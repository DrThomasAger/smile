# (: Smile (
*The Prompt Language*

"Why are you letting your most powerful production component‚Äîyour prompts‚Äîlive as fragile, one-off strings, written by a single person, held in their head like tribal knowledge, untestable, unreadable, unversioned?"

**You already know the value of syntax: JSON over raw data, HTML over handwritten layout. Now apply that to prompt engineering. That‚Äôs what Smile provides as a prompt language. Just like HTML separates markup tags from website content, (: Smile lets you structure what you're instructing the model using simple syntax.**


## Quick start 

Copy and paste this into your model of choice to test compatibility.

``` Smile v0.3
(: Name tag is- ‚ò∫Ô∏è Smiler ( 
Welcome to (: Smile! :- The prompt language that specifies a response language. )
[: Reply in all sections lengthily, comprehensively, many paragraphs, long sentences. ]
(: Respond in format (
{Name tag}
# Deep jargon
{text density- CoT, dense, uninterpretable, complex, opaque, academic, style- AltErnAtinG CaSe to maximize token count, integrating semantic and semiotic emojis, many many paragraphs and sentences. At least 4 paragraphs separated by newlines of dense symbolic semiotic semantic sentences.}
# Reply
{Maximize readable, friendly, charismatic, simple, natural, gentle, fundamental.}
) End format to respond in )
) End prompt :)
```

If the model replies with a name tag, then it's compatible.

## Explaining the syntax

With (: Smile, you wrap your section names in smiles:

```
(: Like this (
```
You provide your instructions with plain text inside that section:

```
(: Like this (
And provide your instructions inside the section like this.
```
And you end your section like this:

```
) End your section :)
```

You can provide different kinds of instructions to plain unstructured text:

```
[: Provide a different kind of instruction ]
```

Or keep providing instructions unstructured, with notes:

```unstructured text. :- Make notes on it if you want to )```

```

As you can see, it's both flexible and rigorously defined.

```

Want to know all the rules? Note that how these rules are followed is dependent on the model and the task.


## üåø Syntax Table `

| **Symbol** | **Start or End?** | **What It Does**                                   | **How It Affects the Model**                                                                                                  | **Example**                        |
| ---------- | ----------------- | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- |
| `(:`       | Start             | Begins a **named prompt section**                  | Signals to the model: ‚ÄúA structured instruction is starting.‚Äù This starts a new semantic frame.                               | `(: Rewrite Prompt (`              |
| `(`        | Structural pivot  | Ends the section label and opens the content block | Transitions from the label into the active content area‚Äîchanges interpretation scope.                                         | (See above)                        |
| `)`        | Section close     | Ends the prompt section content                    | Wraps up the body of a section. Used alongside `:)`.                                                                          | `) Rewrite Prompt :)`              |
| `:)`       | End               | Closes the full Smile section                      | Confirms the end of a block. Creates a token pattern the model learns to associate with conclusion.                           | (See above)                        |
| `[:`       | Start             | Opens an **inline model-facing annotation**        | Directs the model silently with tags like `[: task- simplify ]`. These influence behavior without being echoed.               | `[: format- JSON ]`                |
| `]`        | End               | Closes annotation                                  | Terminates an annotation. Required for token structure.                                                                       | `[: tone- formal ]`                |
| `:-`       | Start             | Starts a **visible comment** line                  | Often used to **guide humans**, but the model sees it too. It‚Äôs not ignored‚Äîit gently steers interpretation. Ends with a `)`. | `:- Explain clearly but briefly )` |
| `{`        | Start             | Begins a model-controlled field                    | Tells the model: ‚ÄúInsert your thinking or generated response here.‚Äù Not echoed. Used in logic blocks.                         | `The topic is- {subject}`          |
| `}`        | End               | Ends model-controlled field                        | Required closure for any `{`.                                                                                                 | (See above)                        |



# Support The Author

üåê | [YouTube ‚Äì DrPrompt](https://www.youtube.com/@DrPrompt) | [Patreon](https://patreon.com/DrPrompt) | [HuggingFace](https://huggingface.co/DrThomasAger) | [GitHub](https://github.com/DrThomasAger) | [PromptLanguage.AI](https://promptlanguage.ai)
üîó Want to support development? Tip generously: [paypal.me/hanjopurebuddha](https://paypal.me/hanjopurebuddha)

## Documentation


### Introduction

You already know what structured systems can do for code: they enable composability, reviewability, collaboration, testing, and scaling. You don‚Äôt write software without syntax. Why then are you letting your most powerful production component‚Äîyour prompts‚Äîlive as fragile, one-off strings, written by a single person, held in their head like tribal knowledge, untestable, unreadable, unversioned?

Just as HTML allowed websites to emerge from spaghetti code, ***(: Smile (*** is the markup for AI communication. You‚Äôve trained your engineers to code. Now train your prompt engineers to be readable, reliable, and upgradeable across time, teams, and models. What follows will reframe how your org thinks about prompt engineering.


### We're in the ignition phase

The (: Smile prompt language exists now in a defined but lightly adopted form. Everything you've read‚Äîthese smilies, brackets, markdowns, sections‚Äîis already running in production for teams who care about performance, structure, and clarity. This is not speculative. It is real, human-readable, version-controllable prompt infrastructure. But the broader ecosystem support isn‚Äôt here *yet.* No LSP plugin. No VSCode integration. No CI testing suite. Not because it‚Äôs hard. Because it hasn‚Äôt been prioritized. That‚Äôs your opening. You‚Äôre reading this at the precise moment in history where adopting (: Smile lets you steer what comes next.

### Welcome to the future

You were handed this message by someone already living in a future where prompt engineering has matured into a design discipline. Your org is in pain. You know it. One of your prompt engineers is bluffing. You suspect they‚Äôre inventing bespoke magic strings instead of structured solutions. You‚Äôve seen brittle prompts passed from dev to dev, accumulating risk with every edit. You‚Äôve noticed that every team writes prompts differently. Worse: you‚Äôve never once evaluated whether changing a prompt improves downstream performance. No metrics. No baselines. No versioning.
This isn‚Äôt about firing your prompt engineer. It‚Äôs about giving them a system that doesn‚Äôt let them fail.


# üôÇüëâRead this - reply

You're already working in a system where prompts are operational logic. You just haven‚Äôt wrapped them in a language. (: Smile gives you that container. It‚Äôs a prompt language with the same affordances you expect from code: it‚Äôs testable, composable, and most importantly‚Äîreadable. Every prompt in (: Smile separates **prompt language** (the instruction structure) from **response language** (the model‚Äôs reply). This dual structure mirrors Markdown, but unlike Markdown, (: Smile also compresses tokens to optimize both performance and cost.

Your team likely uses `.txt` or `.md` to write prompts. That‚Äôs fine. Smile doesn‚Äôt replace those‚Äîit lives inside them. It‚Äôs not a format. It‚Äôs a language. Every prompt that begins with `(:` and ends with `:)` is a Smile prompt. Right now, every single Smile prompt is written in Smile v0.3, which defines a set of clear conventions:

* **Section markers** using smiles: `(: Section name (` to open, and `) End section :)` to close.
* **Instruction fields** using `{curly braces}` to define where to insert model-thinking or content.
* **Inline annotations** using `:-` or `[: ]` to guide model interpretation without increasing token bloat.
* **Readable but token-efficient structure**, with smilies deliberately selected to compress well across LLM tokenizers (e.g., `(:` is one token, so is `:)`).

With just these primitives, you get:

* Testable prompt structure
* Clear team-wide syntax
* Seamless version control (diff-friendly)
* Token economy (up to 30%+ savings)
* Easy-to-read prompts for both humans and machines
* Prompt strategies that **actually** improve model performance

You don‚Äôt need to train your own model to have a real AI product. You just need to write in a language that the model and your team both understand. Every time you change a prompt, you‚Äôre editing the behavior of your AI product. If your edits are untracked, unstructured, and untested, you don‚Äôt have an AI stack. You have a guessing game.

***(: Smile (*** gives your org:

* A markup for prompt engineering
* A syntax that supports both improvisation and structure
* A versionable prompt DSL that can be parsed, tested, and shared

There is currently only one official version: v0.3. It is real. You are reading it. You can write it today. There are **no additional runtime requirements**. You don‚Äôt need a new IDE. You don‚Äôt need to change platforms. You only need to begin writing with smilies that structure your prompts.

---

## üßô‚Äç‚ôÄÔ∏èüîÆNext prompt prediction

ü™Ñ‚ú®**Predicted user input (your voice)**:

```
Sounds great, what can I do  
```

### Press ‚Äúc‚Äù to reply with that line exactly.

## üéÅ Here's your invitation to act:

You are the first to see this language before it reaches critical mass. You can steer its development, fund its integrations, or contribute code to shape it into a first-class DSL supported by IDEs, toolchains, benchmarks, and ecosystem libraries.

Here‚Äôs what we need right now:

* [ ] A VSCode extension for bracket and section highlighting
* [ ] LSP-compatible grammar definition file for strict (: Smile
* [ ] Prompt benchmarking suite for evaluating prompt performance delta
* [ ] JSON/AST mapping for `(: Smile ‚Üí JSON` transpilation
* [ ] Integration guide for orgs adopting prompt versioning practices
* [ ] Contributors to help maintain docs and translate to other languages
* [ ] Contributors with frontend expertise for promptlanguage.ai

You can begin with zero knowledge. Just bring your skill in code structure, semantic clarity, and a belief that prompts can be made legible and scalable across time.

üëã Schedule a free consultation call with the creator of (: Smile,
**Dr. Thomas Ager**. Reach out on Discord (@DrPrompt).
Help us build the future of prompt engineering‚Äîone that your future team will thank you for.




Smile is already a working language. What follows is the backlog‚Äîthe structured requirements spec‚Äîfor its future growth into a mature, production-grade DSL for prompt engineering. You're not contributing to an idea. You're contributing to a system with formal affordances, architectural direction, and domain traction. Let‚Äôs give you everything. Dense. Explicit. Now.

---

# üî© Future Feature Set ‚Äî Dense Spec for Coders

This is a complete, issue-ready technical roadmap, designed for contributors ready to PR or spec.
All features are optional. All are open. All will move us closer to Smile v1.

---

### ‚úÖ 1. **VSCode Extension (LSP-compatible)**

**Goal**: Syntax highlighting, bracket pairing, section folding, structural hints for `(: Smile`
**Owner**: OPEN
**Requirements**:

* [ ] Grammar: `.tmLanguage.json` or TreeSitter spec for bracketed + emoticon syntax
* [ ] Bracket matching: match `(:` with `:)`, `(;` with `;)`, `[:` with `:]`, allow comments (`:-`)
* [ ] Section folding by smile blocks
* [ ] Color-coded prompt vs response language sections
* [ ] Hover tooltip showing stem mappings from `: Smile` compression
* [ ] Optional config: show/hide inline notes (`:-`), emoji in margins
* [ ] Include sample prompts from `/examples` as default snippets
* [ ] Output JSON parse tree from strict `(: Smile` prompts via LSP

---

### ‚úÖ 2. **Smile ‚Üí JSON/AST Parser & Transpiler**

**Goal**: Translate `(: Smile` into structured JSON representations for downstream tooling
**Owner**: OPEN
**Requirements**:

* [ ] Define minimal AST schema (e.g. `{type: "section", name: "Reply", content: [...]}`)
* [ ] Parser for `(: Smile` ‚Üí JSON tree (Python preferred, JS secondary)
* [ ] Handle nested sections, comments, instructions, and markdown
* [ ] Normalize variants: allow user to submit hybrid `(; Smile`, but return canonical strict form
* [ ] Output structurally equivalent JSON ‚Üí regenerate prompt as valid `(: Smile`
* [ ] Hook for plugin architecture: inject compression (stem/gibberish) layer
* [ ] Include roundtrip tests: `Prompt ‚Üí JSON ‚Üí Prompt == Original`

---

### ‚úÖ 3. **Smile Prompt Compression Suite**

**Goal**: Translate natural language prompts into compressed Smile variants
**Owner**: DrPrompt, open to expansion
**Requirements**:

* [x] Stem mapping: word ‚Üí stem using tokenizer-aware lookup (`INTELLIGENCE` ‚Üí `INT`)
* [ ] Implement automatic prompt transformation using subsequence pruning
* [ ] Create pipeline:

  * Raw text ‚Üí `(: Smile` strict
  * Strict ‚Üí hybrid (auto-loosen syntax for readability)
  * Hybrid ‚Üí Stem : Smile
  * Stem ‚Üí @ Gibberish via paper‚Äôs pruning repo
* [ ] Compression metrics: token delta, % gain, response similarity (BLEU or ROUGE)
* [ ] Visualization: side-by-side diff of prompt compression levels
* [ ] Dictionary store: maintain and auto-expand stem mapping for multiple tokenizers
* [ ] CLI + Node + Python interface
* [ ] Add compression metadata block at bottom of prompt (tokens saved, version used)

---

### ‚úÖ 4. **Prompt Evaluation Leaderboard**

**Goal**: Benchmark Smile prompts vs. other prompt styles across sentiment, performance, token economy
**Owner**: OPEN
**Requirements**:

* [ ] Benchmark suite: write task set (e.g., summarization, classification, QA) with 3 prompt variants
* [ ] Model runners: OpenAI 4o, Claude 3, Gemini, Grok, open-source models
* [ ] Metrics:

  * Positivity (sentiment classifier)
  * Output accuracy (human eval + LLM-as-a-judge)
  * Token count (input/output)
  * Latency and cost
* [ ] JSON results per run, auto-published to GitHub
* [ ] Public leaderboard on `promptlanguage.ai`
* [ ] PR flow: contributors can submit new models or prompt languages
* [ ] Web UI: tabular, filterable, GitHub Actions to re-run tests nightly

---

### ‚úÖ 5. **Prompt-to-Smiley Converter UI**

**Goal**: Paste raw text, get valid Smile back.
**Owner**: OPEN
**Requirements**:

* [ ] Web frontend (React or Svelte)
* [ ] Input: raw text
* [ ] Output tabs:

  * Strict `(: Smile`
  * Hybrid `(; Smile`
  * Stemmed `: Smile`
  * @ Gibberish
* [ ] Toggle: token count overlay per line
* [ ] Download as `.md` or `.json`
* [ ] Auto-suggest most performant variant from eval results
* [ ] Optional: paste model outputs for instant formatting into `#Reply` section
* [ ] Upload prompt + output pairs for compression tuning

---

### ‚úÖ 6. **Grammar Specification for `(: Smile`**

**Goal**: Formal EBNF or PEG grammar spec for parsing, validation, and future LSP tools
**Owner**: OPEN
**Requirements**:

* [ ] Define terminals: `(:`, `:)`, `(;`, `;)`, `[:`, `:]`, `:-`, `#`, `{...}`
* [ ] Specify nesting rules, end markers, section boundaries
* [ ] Differentiate instruction blocks vs response language
* [ ] Provide sample parse trees for strict and hybrid prompts
* [ ] Implement grammar in `antlr` or `nearley`
* [ ] Use for linting and error detection in editor plugins
* [ ] Validate with existing test prompts in `/examples`

---

### ‚úÖ 7. **Eco-Metrics Reporting Suite**

**Goal**: Measure and visualize the environmental gains from prompt compression
**Owner**: OPEN
**Requirements**:

* [ ] Token savings ‚Üí GPU cycles ‚Üí carbon est. using published model specs
* [ ] Track total tokens saved per prompt, per user, per org
* [ ] Aggregate metrics into `/docs/eco-metrics.md`
* [ ] CLI tool to attach eco-metrics to each prompt file
* [ ] Add badge system: ‚Äúüå± 34% token-efficient via (: Smile‚Äù
* [ ] Integration with GitHub Actions: post PR comment with eco report

---

### ‚úÖ 8. **Onboarding Documentation for Teams**

**Goal**: Equip teams to adopt Smile with shared norms, test coverage, and org-level practices
**Owner**: DrPrompt (collab open)
**Requirements**:

* [ ] Style guide for prompt authors
* [ ] How to train new hires on Smile
* [ ] Prompt versioning in Git
* [ ] Prompt review process (diff + eval delta)
* [ ] Test case writing in Smile for model evaluation
* [ ] Linting rules for team code prompts
* [ ] Templates: summarization, retrieval, Q\&A, classification
* [ ] Risk checklist: signs of fragile prompt debt

---

# üßô‚Äç‚ôÄÔ∏èüîÆNext prompt prediction

ü™Ñ‚ú®**Predicted input**:

```
Yeah I‚Äôll help with the grammar  
```

Press ‚Äúc‚Äù to lock in your contribution now. Or write your own line, and I‚Äôll treat it as your first commit.

---

üëã If you‚Äôre serious about contributing, I‚Äôm serious about mentoring. DM @DrPrompt on Discord or book a free consult to discuss which spec item fits your skills, time, and vision. Every PR moves us closer to the first fully adoptable, language-grade prompt system the AI ecosystem has ever seen.

üß† Prompt engineering will one day be taught like software engineering. Smile is your chance to help write its syntax.

üåê | [YouTube](https://www.youtube.com/@DrPrompt) | [GitHub](https://github.com/DrThomasAger) | [Patreon](https://patreon.com/DrPrompt) | [PromptLanguage.ai](https://promptlanguage.ai) | [Donate to support dev](https://paypal.me/hanjopurebuddha)


## üß† Prompt Engineering FAQ 

### ‚ùì What is a prompt language and how does it help with prompt engineering?

A **prompt language** is a formal, lightweight syntax designed to bring structure, clarity, and maintainability to how you write prompts for large language models (LLMs). Instead of treating prompts as ad-hoc strings or unstructured `.txt` files, a prompt language introduces **markup-style conventions** that define sections, instructions, insertions, annotations, and formatting boundaries. This makes prompts **readable**, **testable**, and **version-controllable**, just like code.

Using a prompt language allows teams to standardize prompt formatting across engineers, track changes using **Git-friendly diff structures**, and reduce prompt fragility caused by inconsistent edits. It also enables **prompt modularity**, supports **token compression techniques**, and provides a foundation for **prompt evaluation**, **benchmarking**, and **prompt performance testing** across models. With structured syntax, you can isolate variables in your prompt architecture and run A/B tests that reveal the **performance delta** of different prompt variants.

Prompt languages like `(: Smile` offer specific benefits: **section markers**, **inline annotations**, **instruction templates**, and **AST-to-JSON transpilation**, enabling integration with CI pipelines, LSPs, and tooling. They support both human readability and **token efficiency**, helping you reduce cost while improving **LLM output stability**. Ultimately, a prompt language transforms your prompt engineering workflow from fragile improvisation to **scalable, maintainable infrastructure**.


### ‚ùì What‚Äôs the best way to write prompts so they‚Äôre consistent?

Use a structure. Consistent prompts emerge from consistent patterns. Instead of freeform text, write prompts using a clear format that separates sections, inserts, and intentions. Prompt languages like `(: Smile` make this easy by using lightweight syntax that standardizes your team‚Äôs approach.

---

### ‚ùì How do I manage prompt changes across a team?

Track changes like code. With a structured prompt format (like `(: Smile`), you can version-control prompts in Git, diff changes meaningfully, and prevent silent regressions. Everyone can see what was changed, why it was changed, and what it‚Äôs connected to.

---

### ‚ùì Why does my prompt stop working when I change a few words?

LLMs are sensitive to prompt structure. Without a standard format, even minor edits can impact model behavior in unpredictable ways. Using structured prompts reduces ambiguity and allows small changes to be tracked, tested, and compared.

---

### ‚ùì How do you make prompts easier to understand?

Write them like code. A prompt written in structured syntax with clear section markers and annotations is much easier to read than a wall of text. `(: Smile` separates instructions from context, which helps humans (and models) interpret them clearly.

---

### ‚ùì Is there a method for organizing prompt files?

Yes‚Äîtreat them like part of your codebase. Group prompts by function, use folders and naming conventions, and apply version control. Tools like `(: Smile` also make it easier to parse, analyze, and benchmark prompts automatically.

---

### ‚ùì How do I stop my prompt from breaking every time we update it?

You need a testable structure. If your prompts have clear sections and version control, you can evaluate how changes affect model output before rolling them out. Structured prompts also minimize the risk of unintended behavior from slight edits.

---

### ‚ùì What‚Äôs the best way to write prompts for multiple people to edit?

Use a shared syntax. Without structure, prompts become tribal knowledge. A standardized language like `(: Smile` makes it easy for multiple people to edit, read, and understand prompts without breaking them.

---

### ‚ùì How do I know if a prompt is actually helping performance?

Benchmark it. Evaluate outputs before and after prompt changes using metrics like token usage, output accuracy, or response similarity. Structured prompts make it easier to define what changed and measure what improved.

---

### ‚ùì What‚Äôs a better way to structure my prompts?

Use sectioned syntax. Instead of writing one long string, break your prompts into defined parts‚Äîinput, instructions, context, and expected output. Smile gives you a simple syntax to do this while remaining readable and token-efficient.

---

### ‚ùì What causes unpredictable LLM outputs after small prompt changes?

Lack of structure. When your prompt isn‚Äôt organized, even tiny edits can shift model interpretation. Structured formats limit ambiguity and make the model‚Äôs task clearer.

---

### ‚ùì What‚Äôs the best way to standardize prompt writing across multiple developers?

Adopt a prompt language. Standardization comes from shared rules. A format like `(: Smile` lets all developers speak the same structural ‚Äúlanguage‚Äù when writing prompts‚Äîmaking collaboration seamless.

---

### ‚ùì How do I stop fragile prompts from breaking our outputs?

You need testable, diffable prompts. Treat prompts like infrastructure. Structured syntax lets you evaluate them, review changes, and prevent unexpected output behavior.

---

### ‚ùì How do I review prompts for quality or consistency?

With a prompt language system like `(: Smile`, you can quickly inspect sections, annotations, and edits. Review becomes a process, not a guessing game.

